---
title: "Setup and Data preparation"
output: html_notebook
---
Load some libraries

```{r}
library(dplyr) # for data manipulation
library(curl) # data fetching
library(ggplot2) # plotting
library(brms) # modeling
library(tidyr) # for data manipulation
library(hashr) # hashing
library(likert) # plotting likert scales
library(plyr) # data manipulation
```

## Data Preparation
We begin by importing the csv data

```{r}
# d.orig <- read.csv(curl("https://raw.githubusercontent.com/BrokenWindowsInvestigation/Submissions/master/data.csv?token=AB26LHN6X2UD5NOF3RBOKI3AMGIJS"))
#d.orig <- read.csv("/home/gurgy/Projects/RobotResearcher/submissions/data.csv")
#d.orig <- read.csv("C:/Users/namor/source/repos/submissions/data.csv")
d.orig <- read.csv("C:/Users/Hampus/repos/submissions/data.csv")
head(d.orig)
```

We also define some utility functions for data processing

```{r}
# convert categories to int 1..n in the order given
encode.categorical <- function(column, categories) {
  factor(column, level = categories)
}

# convert bool to factor
encode.bool <- function(column) {
  encode.categorical(column, c("true", "false"))
}

# convert ordered categories to ordered factors in the order given
encode.orderedcategorical <- function(column, categories) {
  as.ordered(encode.categorical(column, categories))
}

# Change likert scales to ordered factors
encode.likert <- function(column) {
  encode.orderedcategorical(column, c(-3, -2, -1, 0, 1, 2, 3))
}
```

We use the utility function to set the correct types for each data column and list the columns for manual inspection

```{r}
d <- data.frame(
  session                       = factor(d.orig$session), 
  
  time                          = d.orig$time,
  reused_logic_constructor      = encode.bool(d.orig$reused_logic_constructor),
  reused_logic_validation       = encode.bool(d.orig$reused_logic_validation),
  equals_state                  = encode.orderedcategorical(d.orig$equals_state, c("Not implemented", "Duplicated", "Good")),
  hashcode_state                = encode.orderedcategorical(d.orig$hashcode_state, c("Not implemented", "Duplicated", "Good")),
  
  documentation                 = factor(d.orig$documentation),
  
  var_names_copied_all          = d.orig$var_names_copied_all,
  var_names_copied_good         = d.orig$var_names_copied_good,
  var_names_copied_good.ratio   = d.orig$var_names_copied_good / d.orig$var_names_copied_all,
  var_names_new_all             = d.orig$var_names_new_all,
  var_names_new_good            = d.orig$var_names_new_good,
  var_names_new_good.ratio      = d.orig$var_names_new_good / d.orig$var_names_new_all,
  var_names_edited_all          = d.orig$var_names_edited_all,
  var_names_edited_good         = d.orig$var_names_edited_good,
  var_names_edited_good.ratio   = d.orig$var_names_edited_good / d.orig$var_names_edited_all,
  sonarqube_issues              = d.orig$sonarqube_issues_major + d.orig$sonarqube_issues_minor + d.orig$sonarqube_issues_info + d.orig$sonarqube_issues_critical,
  sonarqube_issues.major       = d.orig$sonarqube_issues_major,
  sonarqube_issues.minor       = d.orig$sonarqube_issues_minor,
  sonarqube_issues.info        = d.orig$sonarqube_issues_info,
  sonarqube_issues.critical    = d.orig$sonarqube_issues_critical,
  
  group                         = factor(d.orig$group),
  education_level               = encode.orderedcategorical(d.orig$education_level, c("None", "Some bachelor studies", "Bachelor degree", "Some master studies", "Master degree", "Some Ph.D. studies", "Ph. D.")),
  education_field               = factor(d.orig$education_field),
  work_domain                   = factor(d.orig$work_domain),
  
  work_experience_programming   = d.orig$work_experience_programming,
  work_experience_java          = d.orig$work_experience_java,
  
  workplace_pair_programming    = encode.bool(d.orig$workplace_pair_programming),
  workplace_peer_review         = encode.bool(d.orig$workplace_peer_review),
  workplace_td_tracking         = encode.bool(d.orig$workplace_td_tracking),
  workplace_coding_standards    = encode.bool(d.orig$workplace_coding_standards),
  
  task_completion               = encode.orderedcategorical(d.orig$task_completion, c("Not submitted", "Does not compile", "Invalid solution", "Completed")),
  
  quality_pre_task              = encode.likert(d.orig$quality_pre_task),
  quality_post_task             = encode.likert(d.orig$quality_post_task),

  modified_lines                = d.orig$modified_lines,
  
  high_debt_version             = encode.bool(d.orig$high_debt_version),
  
  scenario                      = encode.categorical(d.orig$scenario, c("booking", "tickets")),
  
  order                         = encode.orderedcategorical(d.orig$order, c(0, 1)),
  large_structure_change        = encode.bool(d.orig$large_structure_change)
)

str(d)
```

Pick out some subdata sets
```{r}
d.sessions <- d %>% group_by(session) %>% summarise(
  across(task_completion, min),
  across(c(
    education_level, 
    education_field, 
    work_domain, 
    group,
    work_experience_java, 
    work_experience_programming, 
    workplace_coding_standards, 
    workplace_pair_programming, 
    workplace_peer_review, 
    workplace_td_tracking
  ), first)
  ) 

d.sessions.completed <- d.sessions %>% filter(task_completion == "Completed")
d.completed <- d %>% filter(task_completion == "Completed")
d.both_completed <- d %>% semi_join(d.sessions.completed, by = "session")

d$work_experience_programming.s = scale(d$work_experience_programming)
d$work_experience_java.s = scale(d$work_experience_java)

d.completed$work_experience_programming.s = scale(d.completed$work_experience_programming)
d.completed$work_experience_java.s = scale(d.completed$work_experience_java)

d.both_completed$work_experience_programming.s = scale(d.both_completed$work_experience_programming)
d.both_completed$work_experience_java.s = scale(d.both_completed$work_experience_java)
```


Utility function
```{r}

extendable_model <- function(base_name, base_formula, family, data, base_priors = NULL, base_control = NULL) {
  function(additional_variables = NULL, additional_priors = NULL, only_priors = FALSE, sample_prior = "no", control_override = NULL) {
    
    additional_variables.sorted <- sort(additional_variables)
    additional_variables.formula <- paste(additional_variables.sorted, collapse = " + ")
    additional_variables.name <- paste(additional_variables.sorted, collapse = ".")
    
    priors <- base_priors
    if (!is.null(additional_priors)) {
      priors <- c(base_priors, additional_priors)
    }
    if (only_priors) {
      priors <- NULL
    }
    
    formula <- base_formula
    if (!is.null(additional_variables)) {
      formula <- paste(base_formula, additional_variables.formula, sep = " + ")
    }
    
    name <- base_name
    if (!is.null(additional_variables)) {
      name <- paste(base_name, hash(additional_variables.name), sep = ".")
    }
    name <- paste(name, paste("sample_priors-", sample_prior, sep = ""), sep = ".")
    name <- paste(name, paste("priors_hash-", hash(priors), sep = ""), sep = ".")
    name <- paste(name, paste("formula_hash-", hash(formula), sep = ""), sep = ".")
    
    control <- base_control
    if (!is.null(control_override)) {
      control <- control_override
    }
    
    brm(
      formula = as.formula(formula),
      family = family,
      data = as.data.frame(data),
      prior = priors,
      empty = only_priors,
      sample_prior = sample_prior,
      file = paste("fits", name, sep = "/"),
      file_refit = "on_change",
      control = control
    )
  }
}

# m0.with <- extendable_model(
#   base_name = "m0", 
#   base_formula = "time ~ 1",
#   family = negbinomial(), 
#   data = d.both_completed, 
#   base_priors = c(
#     prior(normal(0, 1), class = "Intercept")
#   )
# )
# 
# prior_summary(m0.with(only_priors = TRUE))
# 
# pp_check(m0.with(sample_prior = "only"), nsamples = 200)
# 
# summary(m0.with())
# 
# 
# pp_check(m0.with(), nsamples = 200)
# 
# loo(
#   m0.with(),
#   m0.with("high_debt_version"),
#   m0.with(c("high_debt_version", "scenario"))
# )

```
