---
title: "Self-Reported Submission Quality Model"
author: Hampus Broman & William LevÃ©n
date: 2021-05
output: 
  html_document: 
    pandoc_args: [ "-o", "docs/self-reported_submission_quality.html" ]
---

```{r include-setup, include=FALSE}
# Load setup file
source(knitr::purl('setup.Rmd', output = tempfile()))
```



## Looking at the data

Looks like there is a slightly higher rate of negative ratings for the high debt group, but also an even smaller increase in positive ratings.

```{r}
data.frame(
  "High_debt_version" = d.both_completed %>%
    filter(high_debt_version == "true") %>%
    pull(quality_post_task) %>%
    revalue(c(
      "-3"="Very Bad",
      "-2"="Bad",
      "-1"="Somewhat Bad",
      "0"="Neutral",
      "1"="Somewhat Good",
      "2"="Good",
      "3"="Very Good"
    )),
  "Low_debt_version" = d.both_completed %>%
    filter(high_debt_version == "false") %>%
    pull(quality_post_task) %>%
    revalue(c(
      "-3"="Very Bad",
      "-2"="Bad",
      "-1"="Somewhat Bad",
      "0"="Neutral",
      "1"="Somewhat Good",
      "2"="Good",
      "3"="Very Good"
    ))
) %>%
  likert() %>%
  plot(
    type="density",
    facet = TRUE,
  )
```

## Initial model
As the data is collected from a likert scale we will use a cumulative family, indicating that each level on the scale is an incremental step. This model is also able to fit the data well.

We include `high_debt_verison` as a predictor in our model as this variable represent the very effect we want to measure.
We also include a varying intercept for each individual to prevent the model from learning too much from single participants with extreme measurements.

### Selecting priors {.tabset}

We iterate over the model until we have sane priors.

```{r initial-model-definition, class.source = 'fold-show'}
own_quality.with <- extendable_model(
  base_name = "own_quality",
  base_formula = "quality_post_task ~ 1  + (1 | session)",
    base_priors = c(
    prior(normal(0, 2.5), class = "Intercept"),
    #prior(normal(0, 1), class = "b"),
    prior(exponential(1), class = "sd")
  ),
  family = cumulative(),
  data = d.both_completed,
  base_control = list(adapt_delta = 0.95)
)
```

#### Default priors

```{r default-priors}
prior_summary(own_quality.with(only_priors= TRUE))
```

#### Selected priors

```{r selected-priors, warning=FALSE}
prior_summary(own_quality.with(sample_prior = "only"))
```

#### Prior predictive check

```{r priors-check, warning=FALSE}
pp_check(own_quality.with(sample_prior = "only"), nsamples = 200, type = "bars")
```

#### Beta parameter influence

We choose a beta parameter priors allowing for the beta parameter to account for 100% of the effect but that is skeptical to such strong effects from the beta parameter.

```{r priors-beta, warning=FALSE}
sim.size <- 1000
sim.intercept <- rnorm(sim.size, 0, 2.5)
sim.beta <- rnorm(sim.size, 0, 1)
sim.beta.diff <- (plogis(sim.intercept + sim.beta) / plogis(sim.intercept) * 100) - 100

data.frame(x = sim.beta.diff) %>%
  ggplot(aes(x)) +
  geom_density() +
  xlim(-150, 150) +
  labs(
    title = "Beta parameter prior influence",
    x = "Estimate with beta as % of estimate without beta",
    y = "Density"
  )

```

### Model fit {.tabset}

We check the posterior distribution and can see that the model seems to have been able to fit the data well. 
Sampling seems to also have worked well as Rhat values are close to 1 and the sampling plots look nice.

#### Posterior predictive check

```{r base-pp-check}
pp_check(own_quality.with(), nsamples = 200, type = "bars")
```

#### Summary

```{r base-summary}
summary(own_quality.with())
```

#### Sampling plots

```{r base-plot}
plot(own_quality.with(), ask = FALSE)
```

## Model quality extenstions {.tabset}

```{r mo-priors}
# default prior for monotonic predictor
edlvl_prior <- prior(dirichlet(2), class = "simo", coef = "moeducation_level1")
# deafult beta prior
beta_prior <- prior(normal(0, 1), class = "b")
```

We use `loo` to check some possible extensions on the model.

### Variable names {.tabset}

```{r model-extension-q1, warning=FALSE, class.source = 'fold-show'}
loo_result <- loo(
  # Benchmark model(s)
  own_quality.with(),
  
  # New model(s)
  own_quality.with("var_names_new_good.ratio", beta_prior),
  own_quality.with("var_names_copied_good.ratio", beta_prior),
  own_quality.with(c("var_names_copied_good.ratio", "var_names_new_good.ratio"), beta_prior)
)
```

#### Comparison

```{r model-extension-q1-sum, warning=FALSE}
loo_result[2]
```

#### Diagnostics

```{r model-extension-q1-dig, warning=FALSE}
loo_result[1]
```

### Reuse {.tabset}

```{r model-extension-q2, warning=FALSE, class.source = 'fold-show'}
loo_result <- loo(
  # Benchmark model(s)
  own_quality.with(),
  
  # New model(s)
  own_quality.with("reused_logic_constructor", beta_prior),
  own_quality.with("reused_logic_validation", beta_prior),
  own_quality.with(c("reused_logic_validation", "reused_logic_constructor"), beta_prior)
)
```

#### Comparison

```{r model-extension-q2-sum, warning=FALSE}
loo_result[2]
```

#### Diagnostics

```{r model-extension-q2-dig, warning=FALSE}
loo_result[1]
```

### Utility methods {.tabset}

```{r model-extension-q3, warning=FALSE, class.source = 'fold-show'}
loo_result <- loo(
  # Benchmark model(s)
  own_quality.with(),
  
  # New model(s)
  own_quality.with("equals.exists", beta_prior),
  own_quality.with("hashcode.exists", beta_prior),
  own_quality.with(c("hashcode.state", "equals.exists"), beta_prior)
)
```

#### Comparison

```{r model-extension-q3-sum, warning=FALSE}
loo_result[2]
```

#### Diagnostics

```{r model-extension-q3-dig, warning=FALSE}
loo_result[1]
```

### Other quality attributes {.tabset}

```{r model-extension-q4, warning=FALSE, class.source = 'fold-show'}
loo_result <- loo(
  # Benchmark model(s)
  own_quality.with(),
  
  # New model(s)
  own_quality.with("sonarqube_issues.s"),
  own_quality.with("documentation")
)
```

#### Comparison

```{r model-extension-q4-sum, warning=FALSE}
loo_result[2]
```

#### Diagnostics

```{r model-extension-q4-dig, warning=FALSE}
loo_result[1]
```

## Model predictor extenstions {.tabset}

We create a new base model based on what we learned from extending the previous model with different quality measurements. We take care to not include redundant quality indicators.

```{r initial-model-definition-2, class.source = 'fold-show'}
own_quality1.with <- extendable_model(
  base_name = "own_quality1",
  base_formula = "quality_post_task ~ 1 + 
    var_names_copied_good.ratio + 
    var_names_new_good.ratio + 
    reused_logic_validation + 
    equals.exists + 
    sonarqube_issues.s + 
    documentation + 
    (1 | session)",
    base_priors = c(
    prior(normal(0, 2.5), class = "Intercept"),
    prior(normal(0, 1), class = "b"),
    prior(exponential(1), class = "sd")
  ),
  family = cumulative(),
  data = data.frame(d.both_completed),
  base_control = list(adapt_delta = 0.95)
)
```

### One variable {.tabset}

```{r model-extension-1, warning=FALSE, class.source = 'fold-show'}
loo_result <- loo(
  # Benchmark model(s)
  own_quality.with(),
  own_quality1.with(),
  
  # New model(s)
  own_quality1.with("work_domain"),
  own_quality1.with("work_experience_programming.s"),
  own_quality1.with("work_experience_java.s"),
  own_quality1.with("education_field"),
  own_quality1.with("mo(education_level)", edlvl_prior),
  own_quality1.with("workplace_peer_review"),
  own_quality1.with("workplace_td_tracking"),
  own_quality1.with("workplace_pair_programming"),
  own_quality1.with("workplace_coding_standards"),
  own_quality1.with("scenario"),
  own_quality1.with("group")
)
```

#### Comparison

```{r model-extension-1-sum, warning=FALSE}
loo_result[2]
```

#### Diagnostics

```{r model-extension-1-dig, warning=FALSE}
loo_result[1]
```

### Two variables {.tabset}

```{r model-extension-2, warning=FALSE, class.source = 'fold-show'}
loo_result <- loo(
  # Benchmark model(s)
  own_quality.with(),
  own_quality1.with(),
  
  own_quality1.with("mo(education_level)", edlvl_prior),
  own_quality1.with("education_field"),
  own_quality1.with("workplace_peer_review"),
  
  # New model(s)
  own_quality1.with(c("mo(education_level)", "education_field"), edlvl_prior),
  own_quality1.with(c("mo(education_level)", "workplace_peer_review"), edlvl_prior),
  own_quality1.with(c("education_field", "workplace_peer_review"))
)
```

#### Comparison

```{r model-extension-2-sum, warning=FALSE}
loo_result[2]
```

#### Diagnostics

```{r model-extension-2-dig, warning=FALSE}
loo_result[1]
```

### Three variables {.tabset}

```{r model-extension-3, warning=FALSE, class.source = 'fold-show'}
loo_result <- loo(
  # Benchmark model(s)
  own_quality.with(),
  own_quality1.with(),
  
  own_quality1.with("mo(education_level)", edlvl_prior),
  own_quality1.with("education_field"),
  own_quality1.with("workplace_peer_review"),
  
  own_quality1.with(c("mo(education_level)", "education_field"), edlvl_prior),
  own_quality1.with(c("mo(education_level)", "workplace_peer_review"), edlvl_prior),
  
  # New model(s)
  own_quality1.with(c("mo(education_level)", "education_field", "workplace_peer_review"), edlvl_prior)
)
```

#### Comparison

```{r model-extension-3-sum, warning=FALSE}
loo_result[2]
```

#### Diagnostics

```{r model-extension-3-dig, warning=FALSE}
loo_result[1]
```

## Candidate models  {.tabset}
We pick some of our top performing models as candidates and inspect them closer.

The candidate models are named and listed in order of complexity.

### OwnQuality0  {.tabset}

We select the simplest model as a baseline.

```{r own_quality0, class.source = 'fold-show', warning=FALSE, message=FALSE}
own_quality0 <- brm(
  "quality_post_task ~ 1 + 
    var_names_copied_good.ratio + 
    var_names_new_good.ratio + 
    reused_logic_validation + 
    equals.exists + 
    sonarqube_issues.s + 
    documentation + 
    (1 | session)",
  prior = c(
    prior(normal(0, 2.5), class = "Intercept"),
    prior(normal(0, 1), class = "b"),
    prior(exponential(1), class = "sd")
  ),
  family = cumulative(),
  data = as.data.frame(d.both_completed),
  file = "fits/own_quality0",
  file_refit = "on_change",
  control = list(adapt_delta = 0.95),
  seed = 20210421
)
```

#### Summary

```{r own_quality0-sum}
summary(own_quality0)
```

#### Random effects

```{r own_quality0-raneff}
ranef(own_quality0)
```

#### Sampling plots

```{r own_quality0-plot}
plot(own_quality0, ask = FALSE)
```

#### Posterior predictive check

```{r own_quality0-pp}
pp_check(own_quality0, nsamples = 200, type = "bars")
```

### OwnQuality1  {.tabset}

We select the best performing model with one variable.

```{r own_quality1, class.source = 'fold-show', warning=FALSE, message=FALSE}
own_quality1 <- brm(
  "quality_post_task ~ 1 + 
    var_names_copied_good.ratio + 
    var_names_new_good.ratio + 
    reused_logic_validation + 
    equals.exists + 
    sonarqube_issues.s + 
    documentation + 
    (1 | session) +
    mo(education_level)",
  prior = c(
    prior(normal(0, 2.5), class = "Intercept"),
    prior(normal(0, 1), class = "b"),
    prior(exponential(1), class = "sd"),
    prior(dirichlet(2), class = "simo", coef = "moeducation_level1")
  ),
  family = cumulative(),
  data = as.data.frame(d.both_completed),
  file = "fits/own_quality1",
  file_refit = "on_change",
  control = list(adapt_delta = 0.95),
  seed = 20210421
)
```

#### Summary

```{r own_quality1-sum}
summary(own_quality1)
```

#### Random effects

```{r own_quality1-raneff}
ranef(own_quality1)
```

#### Sampling plots

```{r own_quality1-plot}
plot(own_quality1, ask = FALSE)
```

#### Posterior predictive check

```{r own_quality1-pp}
pp_check(own_quality1, nsamples = 200, type = "bars")
```

### OwnQuality2  {.tabset}

We select the best performing model with two variables.

```{r own_quality2, class.source = 'fold-show', warning=FALSE, message=FALSE}
own_quality2 <- brm(
  "quality_post_task ~ 1 + 
    var_names_copied_good.ratio + 
    var_names_new_good.ratio + 
    reused_logic_validation + 
    equals.exists + 
    sonarqube_issues.s + 
    documentation + 
    (1 | session) +
    mo(education_level) +
    education_field",
  prior = c(
    prior(normal(0, 2.5), class = "Intercept"),
    prior(normal(0, 1), class = "b"),
    prior(exponential(1), class = "sd"),
    prior(dirichlet(2), class = "simo", coef = "moeducation_level1")
  ),
  family = cumulative(),
  data = as.data.frame(d.both_completed),
  file = "fits/own_quality2",
  file_refit = "on_change",
  control = list(adapt_delta = 0.95),
  seed = 20210421
)
```

#### Summary

```{r own_quality2-sum}
summary(own_quality2)
```

#### Random effects

```{r own_quality2-raneff}
ranef(own_quality2)
```

#### Sampling plots

```{r own_quality2-plot}
plot(own_quality2, ask = FALSE)
```

#### Posterior predictive check

```{r own_quality2-pp}
pp_check(own_quality2, nsamples = 200, type = "bars")
```

### OwnQuality3  {.tabset}

We select the best performing model with three variables.

```{r own_quality3, class.source = 'fold-show', warning=FALSE, message=FALSE}
own_quality3 <- brm(
  "quality_post_task ~ 1 + 
    var_names_copied_good.ratio + 
    var_names_new_good.ratio + 
    reused_logic_validation + 
    equals.exists + 
    sonarqube_issues.s + 
    documentation + 
    (1 | session) +
    mo(education_level) +
    education_field +
    workplace_peer_review",
  prior = c(
    prior(normal(0, 2.5), class = "Intercept"),
    prior(normal(0, 1), class = "b"),
    prior(exponential(1), class = "sd"),
    prior(dirichlet(2), class = "simo", coef = "moeducation_level1")
  ),
  family = cumulative(),
  data = as.data.frame(d.both_completed),
  file = "fits/own_quality3",
  file_refit = "on_change",
  control = list(adapt_delta = 0.95),
  seed = 20210421
)
```

#### Summary

```{r own_quality3-sum}
summary(own_quality3)
```

#### Random effects

```{r own_quality3-raneff}
ranef(own_quality3)
```

#### Sampling plots

```{r own_quality3-plot}
plot(own_quality3, ask = FALSE)
```

#### Posterior predictive check

```{r own_quality3-pp}
pp_check(own_quality3, nsamples = 200, type = "bars")
```

## Final model 
All candidate models look nice, none is significantly better than the others, we will proceed the model containing work experince as it otherwise ourd be added in the next step: `own_quality0`


### All data points {.tabset}

Some participants did only complete one scenario. Those has been excluded from the initial dataset to improve sampling of the models. We do however want to use all data we can and will therefore try to fit the model with the complete dataset.

```{r variation.all, class.source = 'fold-show'}
own_quality0.all <- brm(
  "quality_post_task ~ 1 + 
    var_names_copied_good.ratio + 
    var_names_new_good.ratio + 
    reused_logic_validation + 
    equals.exists + 
    sonarqube_issues.s + 
    documentation + 
    (1 | session)",
  prior = c(
    prior(normal(0, 2.5), class = "Intercept"),
    prior(normal(0, 1), class = "b"),
    prior(exponential(1), class = "sd")
  ),
  family = cumulative(),
  data = as.data.frame(d.completed),
  file = "fits/own_quality0.all",
  file_refit = "on_change",
  control = list(adapt_delta = 0.95),
  seed = 20210421
)
```

#### Summary

```{r variation.all-sum}
summary(own_quality0.all)
```

#### Random effects

```{r variation.all-raneff}
ranef(own_quality0.all)
```

#### Sampling plots

```{r variation.all-plot}
plot(own_quality0.all, ask = FALSE)
```

#### Posterior predictive check

```{r variation.all-pp}
pp_check(own_quality0.all, nsamples = 200, type = "bars")
```

### Final model
* Fitting the model to all data point did not significantly damage the model and will be used as is a more fair representation of reality.

This means that our final model, with all data points and experience predictors, is `own_quality0.all`

## Interpreting the model
To begin interpreting the model we look at how it's parameters were estimated. As our research is focused on how the outcome of the model is effected we will mainly analyze the $\beta$ parameters.

### $\beta$ parameters
```{r interpret-beta-plot, warning=FALSE, message=FALSE}
mcmc_areas(own_quality0.all, 
           pars = c(
             "b_var_names_copied_good.ratio", 
             "b_var_names_new_good.ratio", 
             "b_reused_logic_validationfalse",
             "b_equals.existsFALSE",
             "b_sonarqube_issues.s",
             "b_documentationIncorrect",
             "b_documentationNone"
                    ),
           prob = 0.95) + scale_y_discrete() +
  scale_y_discrete(labels=c(
    "Ratio of good copied var names",
    "Ratio of good new var names",
    "Duplicated validation logic",
    "Missing equals implementation",
    "Amount of sonarqube issues",
    "Incorrect documentation",
    "No documentation"
    )) +
  ggtitle("Beta parameters densities in self assesed quality model", subtitle = "Shaded region marks 95% of the density. Line marks the median")
```

As we have a low of effects playing small roles we will simulate two scenarios, one where the developer, according to us does well and one where the developers does not do so well and see if the participant rating approves with us.


```{r interpret-post, message=FALSE, warning=FALSE}
scale_programming_experience <- function(x) {
  (x - mean(d.completed$work_experience_programming))/ sd(d.completed$work_experience_programming)
}
unscale_programming_experience <- function(x) {
  x * sd(d.completed$work_experience_programming) + mean(d.completed$work_experience_programming)
}

post_settings <- data_frame(
  var_names_copied_good.ratio = c(0.9, 0.5),
  var_names_new_good.ratio = c(0.9, 0.5),
  reused_logic_validation = c("true", "false"),
  equals.exists = c("TRUE", "FALSE"),
  sonarqube_issues.s = c(-1, 1),
  documentation = c("Correct", "Incorrect"),
  session = NA
)

post <- posterior_predict(own_quality0.all, newdata = post_settings) %>%
  melt(value.name = "estimate", varnames = c("sample_number", "settings_id")) %>%
  left_join(
    rowid_to_column(post_settings, var= "settings_id"),
    by = "settings_id"
  ) %>%
  mutate(submission = revalue(reused_logic_validation, c("true" = "Good", "false" = "Bad"))) %>%
  select(
    estimate,
    submission
  )

post.nice <- post %>%  mutate_at("estimate", function(x) revalue(as.ordered(x), c(
      "1"="Very Bad",
      "2"="Bad",
      "3"="Somewhat Bad",
      "4"="Neutral",
      "5"="Somewhat Good",
      "6"="Good",
      "7"="Very Good"
    )))

data.frame(
  Bad_submission = post.nice %>%
    filter(submission == "Bad") %>%
    pull(estimate),
  Good_submission = post.nice %>%
    filter(submission == "Good") %>%
    pull(estimate)
) %>%
  likert() %>%
  plot(
    type="density",
    facet = TRUE,
  )

```

```{r effect-size-diff, warning=FALSE, message=FALSE}
post.diff <- post %>% filter(submission == "Bad")
post.diff$estimate = post.diff$estimate -  filter(post, submission == "Good")$estimate

post.diff %>%
  ggplot(aes(x=estimate)) +
  geom_boxplot(quantile_lines = TRUE, quantile_fun = hdi, vline_linetype = 2) +
  labs(
    title = "Submission rating diff",
    subtitle = "Difference as: bad submission rating - good submission rating",
    x = "Rating difference"
  ) +
  scale_y_continuous(breaks = NULL)
```

We can then proceed to calculate some likelihoods:

```{r, class.source = 'fold-show'}
bad_rated_higher <- sum(post.diff < 0)
good_rated_higher <- sum(post.diff > 0)
x <- good_rated_higher / bad_rated_higher
x
```
Participants were `r scales::label_percent()(x - 1)` more likely to rate the bad submission as worse then they were to rate the good submission as worse. 
