---
title: "Reported Scenario Quality Model"
author: Hampus Broman & William LevÃ©n
date: 2021-05
output: 
  html_document: 
    pandoc_args: [ "-o", "docs/reported_scenario_quality.html" ]
---

```{r include-setup, include=FALSE}
# Load setup file
source(knitr::purl('setup.Rmd', output = tempfile()))
```


## Looking at the data  {.tabset}

We plot the data and can see that there is no obvious large difference between the debt levels or scenarios.

### Per debt level

```{r}
data.frame(
  High_debt_version = d.both_completed %>%
    filter(high_debt_version == "true") %>%
    pull(quality_pre_task) %>%
    revalue(c(
      "-3"="Very Bad",
      "-2"="Bad",
      "-1"="Somewhat Bad",
      "0"="Neutral",
      "1"="Somewhat Good",
      "2"="Good",
      "3"="Very Good"
    )),
  Low_debt_version = d.both_completed %>%
    filter(high_debt_version == "false") %>%
    pull(quality_pre_task) %>%
    revalue(c(
      "-3"="Very Bad",
      "-2"="Bad",
      "-1"="Somewhat Bad",
      "0"="Neutral",
      "1"="Somewhat Good",
      "2"="Good",
      "3"="Very Good"
    ))
) %>%
  likert() %>%
  plot(
    type="density",
    facet = TRUE,
  )
```

### Per scenario

```{r}
data.frame(
  Tickets = d.both_completed %>%
    filter(scenario == "tickets") %>%
    pull(quality_pre_task) %>%
    revalue(c(
      "-3"="Very Bad",
      "-2"="Bad",
      "-1"="Somewhat Bad",
      "0"="Neutral",
      "1"="Somewhat Good",
      "2"="Good",
      "3"="Very Good"
    )),
  Booking = d.both_completed %>%
    filter(scenario == "booking") %>%
    pull(quality_pre_task) %>%
    revalue(c(
      "-3"="Very Bad",
      "-2"="Bad",
      "-1"="Somewhat Bad",
      "0"="Neutral",
      "1"="Somewhat Good",
      "2"="Good",
      "3"="Very Good"
    ))
) %>%
  likert() %>%
  plot(
    type="density",
    facet = TRUE,
  )
```

## Initial model
As the data is collected from a likert scale we will use a cumulative family.

We include `high_debt_verison` as a predictor in our model as this variable represent the very effect we want to measure.
We also include a varying intercept for each individual to prevent the model from learning too much from single participants with extreme measurements.



### Selecting Priors {.tabset}

We iterate over the model until we have sane priors.


#### Base model with priors

```{r initial-model-definition, class.source = 'fold-show'}
scenario_quality.with <- extendable_model(
  base_name = "scenario_quality",
  base_formula = "quality_pre_task ~ 1 + high_debt_version + (1 | session)",
  base_priors = c(
    prior(normal(0, 1), class = "b"),
    prior(normal(0, 2), class = "Intercept"),
    prior(exponential(1), class = "sd")
  ),
  family = cumulative(),
  data = d.both_completed,
)
```

#### Default priors

```{r default-priors}
prior_summary(scenario_quality.with(only_priors= TRUE))
```

#### Selected priors

```{r selected-priors, warning=FALSE}
prior_summary(scenario_quality.with(sample_prior = "only"))
```

#### Prior Predictive Check

```{r priors-check, warning=FALSE}
pp_check(scenario_quality.with(sample_prior = "only"), nsamples = 200, type = "bars")
```

#### Beta Parameter Influence

```{r priors-beta, warning=FALSE}
sim.size <- 1000
sim.intercept <- rnorm(sim.size, 0, 2)
sim.beta <- rnorm(sim.size, 0, 1)
sim.beta.diff <- (plogis(sim.intercept + sim.beta) / plogis(sim.intercept) * 100) - 100

data.frame(x = sim.beta.diff) %>%
  ggplot(aes(x)) +
  geom_density() +
  xlim(-150, 150) +
  labs(
    title = "Beta parameter prior influence",
    x = "Estimate with beta as % of estimate without beta",
    y = "Density"
  )

```

### Model fit {.tabset}

We check the posterior distribution and can see that the model seems to have been able to fit the data well. 
Sampling seems to also have worked well as Rhat values are close to 1 and the sampling plots look nice.

#### Posterior Predictive check

```{r base-pp-check}
pp_check(scenario_quality.with(), nsamples = 200, type = "bars")
```

#### Summary

```{r base-summary}
summary(scenario_quality.with())
```

#### Sampling plots

```{r base-plot}
plot(scenario_quality.with(), ask = FALSE)
```

## Model predictor extenstions {.tabset}

```{r mo-priors}
# default prior for monotonic predictor
edlvl_prior <- prior(dirichlet(2), class = "simo", coef = "moeducation_level1")
```

We use `loo` to check some possible extensions on the model.

### One variable {.tabset}

```{r model-extension-1, warning=FALSE, class.source = 'fold-show'}
loo_result <- loo(
  # Benchmark model(s)
  scenario_quality.with(),
  
  # New model(s)
  scenario_quality.with("work_domain"),
  scenario_quality.with("work_experience_programming.s"),
  scenario_quality.with("work_experience_java.s"),
  scenario_quality.with("education_field"),
  scenario_quality.with("mo(education_level)", edlvl_prior),
  scenario_quality.with("workplace_peer_review"),
  scenario_quality.with("workplace_td_tracking"),
  scenario_quality.with("workplace_pair_programming"),
  scenario_quality.with("workplace_coding_standards"),
  scenario_quality.with("scenario"),
  scenario_quality.with("group")
)
```

#### Comparison

```{r model-extension-1-sum, warning=FALSE}
loo_result[2]
```

#### Diagnostics

```{r model-extension-1-dig, warning=FALSE}
loo_result[1]
```

### Two variables {.tabset}

```{r model-extension-2, warning=FALSE, class.source = 'fold-show'}
loo_result <- loo(
  # Benchmark model(s)
  scenario_quality.with(),
  scenario_quality.with("work_experience_programming.s"),
  scenario_quality.with("workplace_coding_standards"),
  scenario_quality.with("workplace_peer_review"),
  scenario_quality.with("work_experience_java.s"),
  
  # New model(s)
  scenario_quality.with(c("work_experience_programming.s", "workplace_coding_standards")),
  scenario_quality.with(c("work_experience_programming.s", "workplace_peer_review")),
  scenario_quality.with(c("work_experience_programming.s", "work_experience_java.s")),
  
  scenario_quality.with(c("workplace_coding_standards", "workplace_peer_review")),
  scenario_quality.with(c("workplace_coding_standards", "work_experience_java.s")),
  
  scenario_quality.with(c("workplace_peer_review", "work_experience_java.s"))
)
```

#### Comparison

```{r model-extension-2-sum, warning=FALSE}
loo_result[2]
```

#### Diagnostics

```{r model-extension-2-dig, warning=FALSE}
loo_result[1]
```

### Three variables {.tabset}

```{r model-extension-3, warning=FALSE, class.source = 'fold-show'}
loo_result <- loo(
  # Benchmark model(s)
  scenario_quality.with(),
  
  scenario_quality.with("work_experience_programming.s"),
  scenario_quality.with("workplace_coding_standards"),
  scenario_quality.with("workplace_peer_review"),
  scenario_quality.with("work_experience_java.s"),
  
  scenario_quality.with(c("work_experience_programming.s", "workplace_coding_standards")),
  scenario_quality.with(c("work_experience_programming.s", "workplace_peer_review")),
  scenario_quality.with(c("workplace_coding_standards", "work_experience_java.s")),
  
  # New model(s)
  scenario_quality.with(c("work_experience_programming.s", "workplace_coding_standards", "workplace_peer_review")),
  scenario_quality.with(c("work_experience_programming.s", "workplace_coding_standards", "work_experience_java.s")),
  scenario_quality.with(c("work_experience_programming.s", "work_experience_java.s", "workplace_peer_review")),
  scenario_quality.with(c("workplace_coding_standards", "work_experience_java.s", "workplace_peer_review"))
)
```

#### Comparison

```{r model-extension-3-sum, warning=FALSE}
loo_result[2]
```

#### Diagnostics

```{r model-extension-3-dig, warning=FALSE}
loo_result[1]
```

## Candidate models  {.tabset}
We pick some of our top performing models as candidates and inspect them closer.

The candidate models are named and listed in order of complexity.

### ScenarioQuality0  {.tabset}

We select the simplest model as a baseline.

```{r scenario_quality0, class.source = 'fold-show', warning=FALSE, message=FALSE}
scenario_quality0 <- brm(
  "quality_pre_task ~ 1 + high_debt_version + (1 | session)",
  prior = c(
    prior(normal(0, 1), class = "b"),
    prior(normal(0, 2), class = "Intercept"),
    prior(exponential(1), class = "sd")
  ),
  family = cumulative(),
  data = as.data.frame(d.both_completed),
  file = "fits/scenario_quality0",
  file_refit = "on_change",
  seed = 20210421
)
```

#### Summary

```{r scenario_quality0-sum}
summary(scenario_quality0)
```

#### Random effects

```{r scenario_quality0-raneff}
ranef(scenario_quality0)
```

#### Sampling plots

```{r scenario_quality0-plot}
plot(scenario_quality0, ask = FALSE)
```

#### Posterior predictive check

```{r scenario_quality0-pp}
pp_check(scenario_quality0, nsamples = 200, type = "bars")
```

### ScenarioQuality1  {.tabset}

We select the best performing model with one variable.

```{r scenario_quality1, class.source = 'fold-show', warning=FALSE, message=FALSE}
scenario_quality1 <- brm(
  "quality_pre_task ~ 1 + high_debt_version + work_experience_programming.s + (1 | session)",
  prior = c(
    prior(normal(0, 1), class = "b"),
    prior(normal(0, 2), class = "Intercept"),
    prior(exponential(1), class = "sd")
  ),
  family = cumulative(),
  data = as.data.frame(d.both_completed),
  file = "fits/scenario_quality1",
  file_refit = "on_change",
  seed = 20210421
)
```

#### Summary

```{r scenario_quality1-sum}
summary(scenario_quality1)
```

#### Random effects

```{r scenario_quality1-raneff}
ranef(scenario_quality1)
```

#### Sampling plots

```{r scenario_quality1-plot}
plot(scenario_quality1, ask = FALSE)
```

#### Posterior predictive check

```{r scenario_quality1-pp}
pp_check(scenario_quality1, nsamples = 200, type = "bars")
```

### ScenarioQuality2  {.tabset}

We select the best performing model with two variables.

```{r scenario_quality2, class.source = 'fold-show', warning=FALSE, message=FALSE}
scenario_quality2 <- brm(
  "quality_pre_task ~ 1 + high_debt_version + work_experience_programming.s + workplace_coding_standards + (1 | session)",
  prior = c(
    prior(normal(0, 1), class = "b"),
    prior(normal(0, 2), class = "Intercept"),
    prior(exponential(1), class = "sd")
  ),
  family = cumulative(),
  data = as.data.frame(d.both_completed),
  file = "fits/scenario_quality2",
  file_refit = "on_change",
  seed = 20210421
)
```

#### Summary

```{r scenario_quality2-sum}
summary(scenario_quality2)
```

#### Random effects

```{r scenario_quality2-raneff}
ranef(scenario_quality2)
```

#### Sampling plots

```{r scenario_quality2-plot}
plot(scenario_quality2, ask = FALSE)
```

#### Posterior predictive check

```{r scenario_quality2-pp}
pp_check(scenario_quality2, nsamples = 200, type = "bars")
```

### ScenarioQuality3  {.tabset}

We select the best performing model with three variables.

```{r scenario_quality3, class.source = 'fold-show', warning=FALSE, message=FALSE}
scenario_quality3 <- brm(
  "quality_pre_task ~ 1 + high_debt_version + work_experience_programming.s + workplace_coding_standards + work_experience_java.s + (1 | session)",
  prior = c(
    prior(normal(0, 1), class = "b"),
    prior(normal(0, 2), class = "Intercept"),
    prior(exponential(1), class = "sd")
  ),
  family = cumulative(),
  data = as.data.frame(d.both_completed),
  file = "fits/scenario_quality3",
  file_refit = "on_change",
  seed = 20210421
)
```

#### Summary

```{r scenario_quality3-sum}
summary(scenario_quality3)
```

#### Random effects

```{r scenario_quality3-raneff}
ranef(scenario_quality3)
```

#### Sampling plots

```{r scenario_quality3-plot}
plot(scenario_quality3, ask = FALSE)
```

#### Posterior predictive check

```{r scenario_quality3-pp}
pp_check(scenario_quality3, nsamples = 200, type = "bars")
```

## Final Model 
All candidate models look nice, none is significantly better than the others, we will proceed the model containing work experince as it otherwise ourd be added in the next step: `scenario_quality1`


### All data points {.tabset}

Some participants did only complete one scenario. Those has been excluded from the initial dataset to improve sampling of the models. We do however want to use all data we can and will therefore try to fit the model with the complete dataset.

```{r variation.all, class.source = 'fold-show'}
scenario_quality1.all <- brm(
  "quality_pre_task ~ 1 + high_debt_version + work_experience_programming.s + (1 | session)",
  prior = c(
    prior(normal(0, 1), class = "b"),
    prior(normal(0, 2), class = "Intercept"),
    prior(exponential(1), class = "sd")
  ),
  family = cumulative(),
  data = as.data.frame(d.completed),
  file = "fits/scenario_quality1.all",
  file_refit = "on_change",
  seed = 20210421
)
```

#### Summary

```{r variation.all-sum}
summary(scenario_quality1.all)
```

#### Random effects

```{r variation.all-raneff}
ranef(scenario_quality1.all)
```

#### Sampling plots

```{r variation.all-plot}
plot(scenario_quality1.all, ask = FALSE)
```

#### Posterior predictive check

```{r variation.all-pp}
pp_check(scenario_quality1.all, nsamples = 200, type = "bars")
```

### Final model
* Fitting the model to all data point did not significantly damage the model and will be used as is a more fair representation of reality.

This means that our final model, with all data points and experience predictors, is `scenario_quality1.all`

## Interpreting the model
To begin interpreting the model we look at how it's parameters were estimated. As our research is focused on how the outcome of the model is effected we will mainly analyze the $\beta$ parameters.

### $\beta$ parameters
```{r interpret-beta-plot, warning=FALSE, message=FALSE}
mcmc_areas(scenario_quality1.all, pars = c("b_high_debt_versionfalse", "b_work_experience_programming.s"), prob = 0.95) + scale_y_discrete() +
  scale_y_discrete(labels=c("High debt version: false", "Professional programming experience")) +
  ggtitle("Beta parameters densities in scenario quality model", subtitle = "Shaded region marks 95% of the density. Line marks the median")
```

```{r effect-size-1, message=FALSE, warning=FALSE}
scale_programming_experience <- function(x) {
  (x - mean(d.completed$work_experience_programming))/ sd(d.completed$work_experience_programming)
}
unscale_programming_experience <- function(x) {
  x * sd(d.completed$work_experience_programming) + mean(d.completed$work_experience_programming)
}

post_settings <- expand.grid(
  high_debt_version = c("false", "true"),
  session = NA,
  work_experience_programming.s = sapply(c(0, 3, 10, 25, 40), scale_programming_experience)
)

post <- posterior_predict(scenario_quality1.all, newdata = post_settings) %>%
  melt(value.name = "estimate", varnames = c("sample_number", "settings_id")) %>%
  left_join(
    rowid_to_column(post_settings, var= "settings_id"),
    by = "settings_id"
  ) %>%
  mutate(work_experience_programming = unscale_programming_experience(work_experience_programming.s)) %>%
  select(
    estimate,
    high_debt_version,
    work_experience_programming
  )%>%
  mutate(estimate = estimate)

post.nice <- post %>%  mutate_at("estimate", function(x) revalue(as.ordered(x), c(
      "1"="Very Bad",
      "2"="Bad",
      "3"="Somewhat Bad",
      "4"="Neutral",
      "5"="Somewhat Good",
      "6"="Good",
      "7"="Very Good"
    )))

data.frame(
  High_debt_version_3_years = post.nice %>%
    filter(high_debt_version == "true", work_experience_programming == 3) %>%
    pull(estimate),
  Low_debt_version_3_years = post.nice %>%
    filter(high_debt_version == "false", work_experience_programming == 3) %>%
    pull(estimate)
) %>%
  likert() %>%
  plot(
    type="density",
    facet = TRUE,
  )

data.frame(
  High_debt_version_25_years = post.nice %>%
    filter(high_debt_version == "true", work_experience_programming == 25) %>%
    pull(estimate),
  Low_debt_version_25_years = post.nice %>%
    filter(high_debt_version == "false", work_experience_programming == 25) %>%
    pull(estimate)
) %>%
  likert() %>%
  plot(
    type="density",
    facet = TRUE,
  )



```
```{r effect-size-diff, warning=FALSE, message=FALSE}
post.diff <- post %>% filter(high_debt_version == "true")
post.diff$estimate = post.diff$estimate -  filter(post, high_debt_version == "false")$estimate

post.diff %>%
  ggplot(aes(x=estimate)) +
  geom_boxplot(quantile_lines = TRUE, quantile_fun = hdi, vline_linetype = 2) +
  facet_grid(rows = vars(work_experience_programming)) +
  labs(
    title = "Scenario rating diff / years of programming experience",
    subtitle = "Difference as: high debt rating - low debt rating",
    x = "Rating difference"
  ) +
  scale_y_continuous(breaks = NULL)
```


```{r}
print("cases with high debt rated lower:")
length(pull(filter(post.diff, work_experience_programming == 10, estimate < 0), estimate)) / length(pull(filter(post.diff, work_experience_programming == 10), estimate))
print("cases with low debt rated lower:")
length(pull(filter(post.diff, work_experience_programming == 10, estimate > 0), estimate)) / length(pull(filter(post.diff, work_experience_programming == 10), estimate))
```
We see that the high debt version is ranked as worse in 63 % of cases while the low debt version is ranked as worse in the 21 % of cases.