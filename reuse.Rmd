---
title: "Reuse"
output: html_notebook
---
## Looking at the data
There appears to be significant difference in the reuse rate between high and low debt versions.

```{r}
d.completed %>%
  ggplot(aes(high_debt_version)) +
  geom_bar(aes(fill = reused_logic_constructor), position = "fill") + 
  scale_fill_manual("legend", values = c("true" = "green", "false" = "red")) +
  labs(title = "Constructor Reuse") +
  xlab("High Debt version") +
  ylab("Ratio of reuse")

d.completed %>%
  ggplot(aes(high_debt_version)) +
  geom_bar(aes(fill = reused_logic_validation), position = "fill") + 
  scale_fill_manual("legend", values = c("true" = "green", "false" = "red")) +
  labs(title = "Validation Reuse") +
  xlab("High Debt version") +
  ylab("Ratio of reuse")

```
## Descriptive Statistics
Full set reuse of constructor:
```{r}
d.both_completed %>%
  pull(reused_logic_constructor) %>% 
  summary()

```

Full set reuse of validation:
```{r}
d.both_completed %>%
  pull(reused_logic_validation) %>% 
  summary()

```
## Initial model
For a boolean outcome, bernoulli is the most suitable family.

We include `high_debt_verison` as well as a varying intercept for each individual in our initial model. Since they may correlate, constructor and logic reuse are both included in a single multivariate model.

### Selecting Priors
We iterate over the model until we have sane priors, in this case a prior giving a 50/50 chance was chosen in both cases. The prior "lkj(2)" will mean the model is sceptical of strong correlations.

```{r}
reuse0.with <- extendable_model(
  base_name = "reuse0",
  base_formula = "mvbind(
    reused_logic_validation,
    reused_logic_constructor
  ) ~ 1 + high_debt_version + (1 |c| session)",
    base_priors = c(
    prior(normal(0, 1), class = "b"),
    prior(normal(0, 1), class = "Intercept"),
    prior(exponential(1), class = "sd", resp = reusedlogicconstructor),
    prior(exponential(1), class = "sd", resp = reusedlogicvalidation),
    prior(lkj(2), class = "L")
  ),
  family = bernoulli(),
  data = d.both_completed,
  base_control = list(adapt_delta = 0.95)
)

# Default priors:
prior_summary(reuse0.with(only_priors= TRUE))

# Our priors:
prior_summary(reuse0.with(sample_prior = "only"))

# Prior predictive checks
pp_check(reuse0.with(sample_prior = "only"), type = "bars", nsamples = 200, resp = "reusedlogicconstructor")

pp_check(reuse0.with(sample_prior = "only"), type = "bars", nsamples = 200, resp = "reusedlogicvalidation")


```

### Model fit
We check the posterior distribution and can see that the model seems to have been able to fit the data well
```{r}
# Posterior predictive check
pp_check(reuse0.with(), type = "bars", nsamples = 200, resp = "reusedlogicconstructor")

pp_check(reuse0.with(), type = "bars", nsamples = 200, resp = "reusedlogicvalidation")

summary(reuse0.with())
```

## Model extenstions

```{r}
summary(reuse0.with("work_domain"))
```

```{r}
summary(reuse0.with("work_experience_programming.s"))
```


```{r}
summary(reuse0.with("work_experience_java.s"))
```

```{r}
summary(reuse0.with("education_field"))
```

```{r}
summary(reuse0.with(
  "mo(education_level)", 
  c(
    prior(dirichlet(2), class = "simo", coef = "moeducation_level1", resp = "reusedlogicconstructor"),
    prior(dirichlet(2), class = "simo", coef = "moeducation_level1", resp = "reusedlogicvalidation")
  )
))
```

```{r}
summary(reuse0.with("workplace_peer_review"))
```

```{r}
summary(reuse0.with("workplace_td_tracking"))
```

```{r}
summary(reuse0.with("workplace_pair_programming"))
```

```{r}
summary(reuse0.with("workplace_coding_standards"))
```

```{r}
summary(reuse0.with("scenario"))
```

```{r}
summary(reuse0.with("group"))
```

Comparing some extensions with loo.

```{r}
loo(
  reuse0.with(),
  reuse0.with("work_domain"),
  reuse0.with("work_experience_programming.s"),
  reuse0.with("work_experience_java.s"),
  reuse0.with("education_field"),
  reuse0.with(
  "mo(education_level)", 
    c(
    prior(dirichlet(2), class = "simo", coef = "moeducation_level1", resp = "reusedlogicconstructor"),
    prior(dirichlet(2), class = "simo", coef = "moeducation_level1", resp = "reusedlogicvalidation")
  )
  ),
  reuse0.with("workplace_peer_review"),
  reuse0.with("workplace_td_tracking"),
  reuse0.with("workplace_pair_programming"),
  reuse0.with("workplace_coding_standards"),
  reuse0.with("scenario"),
  reuse0.with("group"),
  reuse0.with(
  c("work_domain", 
    "work_experience_programming.s",
    "work_experience_java.s",
    "education_field",
    "mo(education_level)",
    "workplace_peer_review",
    "workplace_td_tracking",
    "workplace_pair_programming",
    "workplace_coding_standards",
    "scenario",
    "group"
    ),
    c(
    prior(dirichlet(2), class = "simo", coef = "moeducation_level1", resp = "reusedlogicconstructor"),
    prior(dirichlet(2), class = "simo", coef = "moeducation_level1", resp = "reusedlogicvalidation")
  )
)
)


```

## Candidate models
We inspect some of our top performing models. 

All models seems to have sampled nicely (rhat = 1 and fluffy plots) they also have about the same fit to the data end similar estimates for the high_debt_version beta parameter

```{r}
reuse1 <-   reuse0.with()
summary(reuse1)
ranef(reuse1)

plot(reuse1)
pp_check(reuse1, type = "bars", nsamples = 100, resp = "reusedlogicconstructor")
pp_check(reuse1, type = "bars", nsamples = 100, resp = "reusedlogicvalidation")
```

```{r}
reuse2 <-   reuse0.with("workplace_peer_review")
summary(reuse2)
ranef(reuse2)
plot(reuse2)
pp_check(reuse2, type = "bars", nsamples = 100, resp = "reusedlogicconstructor")
pp_check(reuse2, type = "bars", nsamples = 100, resp = "reusedlogicvalidation")
```

```{r}
reuse3 <- reuse0.with("workplace_coding_standards")
summary(reuse3)
ranef(reuse3)
plot(reuse3)
pp_check(reuse3, type = "bars", nsamples = 100, resp = "reusedlogicconstructor")
pp_check(reuse3, type = "bars", nsamples = 100, resp = "reusedlogicvalidation")
```

```{r}
reuse4 <-   reuse0.with("workplace_pair_programming")
summary(reuse4)
ranef(reuse4)
plot(reuse4)
pp_check(reuse4, type = "bars", nsamples = 100, resp = "reusedlogicconstructor")
pp_check(reuse4, type = "bars", nsamples = 100, resp = "reusedlogicvalidation")
```

```{r}
reuse34 <-   reuse0.with(c("workplace_pair_programming","workplace_coding_standards"))
summary(reuse34)
ranef(reuse34)
plot(reuse34)
pp_check(reuse34, type = "bars", nsamples = 100, resp = "reusedlogicconstructor")
pp_check(reuse34, type = "bars", nsamples = 100, resp = "reusedlogicvalidation")
```

```{r}
reuse23 <-   reuse0.with(c("workplace_peer_review","workplace_coding_standards"))
summary(reuse23)
ranef(reuse23)
plot(reuse23)
pp_check(reuse23, type = "bars", nsamples = 100, resp = "reusedlogicconstructor")
pp_check(reuse23, type = "bars", nsamples = 100, resp = "reusedlogicvalidation")
```

```{r}
reuse42 <-   reuse0.with(c("workplace_peer_review","workplace_pair_programming"))
summary(reuse23)
ranef(reuse23)
plot(reuse23)
pp_check(reuse23, type = "bars", nsamples = 100, resp = "reusedlogicconstructor")
pp_check(reuse23, type = "bars", nsamples = 100, resp = "reusedlogicvalidation")
```

```{r}
reuse234 <-   reuse0.with(c("workplace_peer_review","workplace_pair_programming","workplace_coding_standards"))
summary(reuse23)
ranef(reuse23)
plot(reuse23)
pp_check(reuse23, type = "bars", nsamples = 100, resp = "reusedlogicconstructor")
pp_check(reuse23, type = "bars", nsamples = 100, resp = "reusedlogicvalidation")
```

```{r}
loo(reuse1,reuse2,reuse3,reuse4,reuse23,reuse34,reuse42,reuse234)
```

### Final model
Verdict: while candidates 2,3,4 are rated slightly better by loo, its very minor, and well within the margin of error. aNY When adding multiple of the factors that increased loo score on its own, the model instead performs worse than the most basic one. We chose reuse1, the model using only session and debt level as predictors.

```{r}
reuse0_with_c <- brm(
  mvbind(
    reused_logic_validation,
    reused_logic_constructor
  ) ~ 1 + high_debt_version + (1 | c | session),
    prior = c(
    prior(normal(0, 1), class = "b"),
    prior(normal(0, 1), class = "Intercept"),
    prior(exponential(1), class = "sd", resp = reusedlogicconstructor),
    prior(exponential(1), class = "sd", resp = reusedlogicvalidation),
    prior(lkj(2), class = "L")
  ),
  family = bernoulli(),
  data = d.both_completed,
  control = list(adapt_delta = 0.95)
)

loo(reuse0.with(), reuse0_with_c)
```


### Model with with incomplete data points

Some participants only completed one scenario. Those has been excluded from the initial dataset to improve sampling of the models. We do however want to use all data we can and will therefore try to fit the model with the complete dataset.

```{r}
reuse0.all <- brm(mvbind(
    reused_logic_validation,
    reused_logic_constructor
  ) ~ 1 + high_debt_version + (1 | c |session),
    prior = c(
    prior(normal(0, 1), class = "b"),
    prior(normal(0, 1), class = "Intercept"),
    prior(exponential(1), class = "sd", resp = reusedlogicconstructor),
    prior(exponential(1), class = "sd", resp = reusedlogicvalidation),
    prior(lkj(2), class = "L")
  ),
  family = bernoulli(),
  data = as.data.frame(d.completed),
  control = list(adapt_delta = 0.95),
  file = "fits/reuse0.all",
  file_refit = "on_change"
)

summary(reuse0.all)
ranef(reuse0.all)
plot(reuse0.all, ask = FALSE)

pp_check(reuse0.all, type = "bars", nsamples = 200, resp = "reusedlogicconstructor")

pp_check(reuse0.all, type = "bars", nsamples = 200, resp = "reusedlogicvalidation")

```

Training the model on all data points reduces does increase the uncertainity. Inlcude or not?

## Interpreting the model

Extract posterior samples:

```{r}

reuse_validation_post <- posterior_predict(reuse0.with(), newdata = data.frame(high_debt_version = c("false", "true"), session = NA), resp ="reusedlogicvalidation")

reuse_validation_post.low <-  reuse_validation_post[,1]
reuse_validation_post.high <-  reuse_validation_post[,2]

mean(reuse_validation_post.low)
mean(reuse_validation_post.high)

reuse_constructor_post <- posterior_predict(reuse0.with(), newdata = data.frame(high_debt_version = c("false", "true"), session = NA), resp ="reusedlogicconstructor")

reuse_constructor_post.low <-  reuse_constructor_post[,1]
reuse_constructor_post.high <-  reuse_constructor_post[,2]

mean(reuse_constructor_post.low)
mean(reuse_constructor_post.high)



```
## Constructor reuse

```{r}

reuse_constructor_post.diff <- reuse_constructor_post.high - reuse_constructor_post.low

data.frame(x = reuse_constructor_post.diff) %>%
  ggplot() +
  geom_histogram(aes(x=x), stat = "count") +
  labs(
    title = "Diff of constructor reuse",
    subtitle = "Difference = High Debt reuse - Low Debt reuse",
    x ="Reuse difference",
    y = "Observations"
  ) +
  theme_minimal() +
  scale_y_continuous(breaks = NULL)

reuse_hypo <- hypothesis(reuse1, "reusedlogicvalidation_high_debt_versionfalse < 0")

# Portion of cases improved by low debt
sum(reuse_constructor_post.diff > 0) / length(reuse_constructor_post.diff)

# Portion of cases made worse by low debt
sum(reuse_constructor_post.diff < 0) / length(reuse_constructor_post.diff)

# Net reuse gain caused by low debt
sum(reuse_constructor_post.diff > 0) / length(reuse_constructor_post.diff) - sum(reuse_constructor_post.diff < 0) / length(reuse_constructor_post.diff)

```

## Validation reuse

```{r}

reuse_validation_post.diff <- reuse_validation_post.high - reuse_validation_post.low

data.frame(x = reuse_validation_post.diff) %>%
  ggplot() +
  geom_histogram(aes(x=x), stat = "count") +
  labs(
    title = "Validation reuse diff",
    subtitle = "Difference = High Debt reuse - Low Debt reuse",
    x ="Reuse difference",
    y = "Observations"
  ) +
  theme_minimal() +
  scale_y_continuous(breaks = NULL)

reuse_hypo <- hypothesis(reuse1, "reusedlogicvalidation_high_debt_versionfalse < 0")

# Portion of cases improved by low debt
sum(reuse_validation_post.diff > 0) / length(reuse_validation_post.diff)

# Portion of cases made worse by low debt
sum(reuse_validation_post.diff < 0) / length(reuse_validation_post.diff)

# Net reuse gain caused by low debt
sum(reuse_validation_post.diff > 0) / length(reuse_validation_post.diff) - sum(reuse_validation_post.diff < 0) / length(reuse_validation_post.diff)

```